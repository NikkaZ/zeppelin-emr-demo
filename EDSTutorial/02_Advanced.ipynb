{
  "metadata": {
    "name": "02_Advanced",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "List of functions shown in this notebook:\n - create a new dataframe based on the existing one\n - SQL Create Table, Alter Table\n - SQL Create temporary view\n - SQL Delete \u003d filter columns\n - SQL Joins, Inner, Left, Right, Full (Outer), Self\n - SQL Union\n - SQL Null Values\n - SQL In, Between\n\nList of functions that can be achieved in different ways:\n - SQL Insert Into (example needed)\n - SQL Update (example needed)\n - SQL Select Into (select + create a new dataframe)\n - SQL Nested Queries (doesn\u0027t support subqueries in WHERE clause but can achieved in different ways)\n - SQL Drop Table (overwrite)\n \nList of functions to have a look at:\n - SQL Pass-Thru\n - SQL Index\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%pyspark\ndf \u003d runquery(\"select * from testNikka\")"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%pyspark\nfrom pyspark.sql.functions import trim\nfrom pyspark.sql.functions import col\n\nprint(\"*** Each result can be shown or saved in the same or another dataframe\")\ndf.groupby(\u0027card_brand\u0027).count().show()\n\nprint(\"*** New dataframe with changed data (trim)\")\ndf1 \u003d df.withColumn(\"card_brand\", trim(col(\"card_brand\")))\ndf1.groupby(\u0027card_brand\u0027).count().show()\n\nprint(\"*** For filters we can apply one filter, save the result in another dataframe and then apply another filter\")\ndf1 \u003d df.filter((\u0027card_brand \u003d \"VISA\"\u0027))\ndf1.groupby(\u0027card_brand\u0027,\u0027lu_name\u0027).count().show()\ndf1.select(\u0027card_brand\u0027,\u0027lu_name\u0027).filter(\u0027lu_name \u003d \"TE37696\" or lu_name \u003d \"TE33806\"\u0027).drop_duplicates([\u0027card_brand\u0027]).show() \n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%pyspark\nfrom pyspark.sql.functions import lower, upper\n\ntable_tolook \u003d \"test\"\ntable_list\u003dshowtables()\ntable_name\u003dtable_list.filter(lower(table_list.TABLE_NAME)\u003d\u003dtable_tolook).collect()\nif len(table_name)\u003e0:\n    print(\"table found \" + table_tolook)\nelse:\n    print(\"table not found \" + table_tolook)\n    \ntable_tolook \u003d \"testtest\"\ntable_name\u003dtable_list.filter(lower(table_list.TABLE_NAME)\u003d\u003dtable_tolook).collect()\nif len(table_name)\u003e0:\n    print(\"table found \" + table_tolook)\nelse:\n    print(\"table not found \" + table_tolook)"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%pyspark\nfrom pyspark.sql.functions import col\ndf \u003d runquery(\"select * from testNikka\")\ndftable \u003d df.select(\"lu_name\", \"card_brand\", \"dcc_bill_amount\").groupBy(\"lu_name\", \"card_brand\").sum(\"dcc_bill_amount\").select(\"lu_name\", \"card_brand\", col(\u0027sum(dcc_bill_amount)\u0027).alias(\u0027sumAmount\u0027))\ncreatetablefromdf(dftable, \"NZ_test\",\"overwrite\")\ndf_NZ \u003d runquery(\"select * from NZ_test\")\nprint(\"*** Overwrite table if exists\")\nprint(df_NZ.count())\n# append: Append contents of this :class:DataFrame to existing data.\n# overwrite: Overwrite existing data.\n# ignore: Silently ignore this operation if data already exists.\n# error (default case): Throw an exception if data already exists\ncreatetablefromdf(dftable, \"NZ_test\",\"append\")\ndf_NZ \u003d runquery(\"select * from NZ_test\")\nprint(\"*** Append to an existing table if exists\")\nprint(df_NZ.count())"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%pyspark\nimport pyspark.sql.functions as func\ndftable \u003d df.select(\"lu_name\", \"card_brand\",\"transaction_date_time\", \"dcc_bill_amount\", \"transaction_amount\") \\\n        .groupBy(\"lu_name\", \"card_brand\",\"transaction_date_time\").sum(\"dcc_bill_amount\",\"transaction_amount\") \\\n        .select(\"lu_name\", \"card_brand\",\"transaction_date_time\", func.col(\u0027sum(dcc_bill_amount)\u0027).alias(\u0027sumAmount\u0027), col(\u0027sum(transaction_amount)\u0027).alias(\u0027sumTransaction\u0027))\ncreatetablefromdf(dftable, \"NZ_test\",\"overwrite\")\ndf_NZ \u003d runquery(\"select * from NZ_test\")\ndf_NZ.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%pyspark\ndf_drop \u003d df_NZ.drop(df_NZ.sumAmount)\ndf_drop.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%pyspark\nfrom pyspark.sql.functions import trim\ndf_union1 \u003d df.withColumn(\"card_brand\", trim(col(\"card_brand\")))\ndf_union2 \u003d df_union1.select(\"lu_name\", \"card_brand\", \"dcc_bill_amount\",\"transaction_amount\").groupby(\"lu_name\", \"card_brand\").sum(\"dcc_bill_amount\",\"transaction_amount\").where(df_union1.card_brand  \u003d\u003d \"VISA\")\ndf_union3 \u003d df_union1.select(\"lu_name\", \"card_brand\", \"dcc_bill_amount\",\"transaction_amount\").groupby(\"lu_name\", \"card_brand\").sum(\"dcc_bill_amount\",\"transaction_amount\").where(df_union1.card_brand  \u003d\u003d \"MASTER\")\ndf_union2.show()\ndf_union3.show()\ndf_concat \u003d df_union2.union(df_union3)\ndf_concat.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%pyspark\ncolumns \u003d [\"id\",\"Name\", \"Surname\"]\ndata \u003d [(\"1\", \"John\", \"Smith\"),\n    (\"2\", \"Paul\", \"Smith\"),\n    (\"4\", \"Tom\", \"Cruise\")]\n\ndf1 \u003d spark.createDataFrame(data\u003ddata,schema\u003dcolumns)\n\ndf1.show()\ncreatetablefromdf(df1, \"NZ_df1\",\"overwrite\")\n\ncolumns \u003d [\"id\", \"Profession\", \"Salary\"]\ndata \u003d [(\"1\", \"Lawyer\", \"20000\"),\n    (\"3\", \"Teacher\", \"10000\"),\n    (\"4\", \"Actor\", \"30000\")]\n\ndf2 \u003d spark.createDataFrame(data\u003ddata,schema\u003dcolumns)\n\ndf2.show()\ncreatetablefromdf(df2, \"NZ_df2\",\"overwrite\")\n\ncolumns \u003d [\"id\", \"City\"]\ndata \u003d [(\"1\", \"Edinburgh\"),\n    (\"2\", \"Inverness\"),\n    (\"4\", \"North Berwick\")]\n\ndf3 \u003d spark.createDataFrame(data\u003ddata,schema\u003dcolumns)\n\ndf3.show()\ncreatetablefromdf(df3, \"NZ_df3\",\"overwrite\")"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%pyspark\nprint(\"*** inner \u003d condition\")\ndf_join  \u003d df1.join(df2, df1.id \u003d\u003d df2.id, \u0027inner\u0027)\ndf_join.show()\n\nprint(\"*** inner \u003e condition\")\ndf_join  \u003d df1.join(df2, df1.id \u003e df2.id, \u0027inner\u0027)\ndf_join.show()\n\nprint(\"*** full outer\")\ndf_join  \u003d df1.join(df2, df1.id \u003d\u003d df2.id, \u0027outer\u0027)\ndf_join.show()\n\nprint(\"*** left\")\ndf_join  \u003d df1.join(df2, df1.id \u003d\u003d df2.id, \u0027left\u0027)\ndf_join.show()\n\nprint(\"*** right\")\ndf_join  \u003d df1.join(df2, df1.id \u003d\u003d df2.id, \u0027right\u0027)\ndf_join.show()\n\nprint(\"*** self\")\ndf_join  \u003d df1.join(df1, [df1.Surname \u003d\u003d df1.Surname, df1.id !\u003d df1.id], \u0027inner\u0027)\ndf_join.show()\n\nprint(\"*** self with alias\")\ndf_as1 \u003d df1.alias(\"df_as1\")\ndf_as2 \u003d df1.alias(\"df_as2\")\ndf_join  \u003d df_as1.join(df_as2, [df_as1.Surname \u003d\u003d df_as2.Surname, df_as1.id !\u003d df_as2.id], \u0027inner\u0027)\ndf_join.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%pyspark\nfrom pyspark.sql.functions import col \n\nprint(\"*** left_anti it selects all rows from df1 that are not present in df2\")\ndf_join  \u003d df1.join(df2, df1.id \u003d\u003d df2.id, \u0027left_anti\u0027)\ndf_join.show()\n\n\nprint(\"*** left_semi unlike the left outer join, the result contains only the columns brought by the left dataset\")\ndf_join  \u003d df1.join(df2, df1.id \u003d\u003d df2.id, \u0027left_semi\u0027)\ndf_join.show()\n\n\nprint(\"*** cross join\")\ndf_join \u003d df1.crossJoin(df2)\ndf_join.show()\n\nprint(\"*** join multiple tables first method\")\ndf1.createOrReplaceTempView(\u0027df1\u0027)\ndf2.createOrReplaceTempView(\u0027df2\u0027)\ndf3.createOrReplaceTempView(\u0027df3\u0027)\nquery \u003d \"select df1.*, df2.profession, df2.salary, df3.city \" \\\n        \"from df1 \" \\\n        \"left join df2 on df1.id\u003ddf2.id \"\\\n        \"left join df3 on df1.id\u003ddf3.id\"\njoined_df \u003d spark.sql(query)\njoined_df.show()\n\nprint(\"*** join multiple tables second method\")\ndf1 \u003d df1.alias(\u0027df1\u0027)\ndf2 \u003d df2.alias(\u0027df2\u0027)\ndf3 \u003d df3.alias(\u0027df3\u0027)\n\njoined_df \u003d df1.join(df2, col(\u0027df1.id\u0027) \u003d\u003d col(\u0027df2.id\u0027), \u0027left\u0027) \\\n    .join(df3, col(\u0027df1.id\u0027) \u003d\u003d col(\u0027df3.id\u0027), \u0027left\u0027) \njoined_df.show()\n\nprint(\"*** join multiple tables second method with select\")\njoined_df \u003d df1.join(df2, col(\u0027df1.id\u0027) \u003d\u003d col(\u0027df2.id\u0027), \u0027left\u0027) \\\n    .join(df3, col(\u0027df1.id\u0027) \u003d\u003d col(\u0027df3.id\u0027), \u0027left\u0027) \\\n    .select(\u0027df1.*\u0027, \u0027df2.profession\u0027, \u0027df2.salary\u0027, \u0027df3.city\u0027)\njoined_df.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%pyspark\nfrom pyspark.sql.functions import col \ndf_join  \u003d df1.join(df2, df1.id \u003d\u003d df2.id, \u0027outer\u0027).select(\"Name\", \"Surname\", \"Profession\", \"Salary\")\nprint(\"*** initial table\")\ndf_join.show()\n\nprint(\"*** filter where name is null\")\ndf_join.where(col(\"Name\").isNull()).show()\n\nprint(\"*** filter where name is not null\")\ndf_join.where(col(\"Name\").isNotNull()).show()\n\nprint(\"*** drop rows where there is a null\")\ndf_join.na.drop().show()\n\nprint(\"*** drop rows where there is a null in the name column\")\ndf_join.na.drop(subset\u003d[\"Name\"]).show()\n\nprint(\"*** replace null value\")\ndf_join.na.fill(\"empty\").show()\ndf_join.na.fill(\"\").show()"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%pyspark\nfrom pyspark.sql.functions import lit, when\n\nprint(\"*** add new column\")\ndf_addcol \u003d df_concat.withColumn(\u0027new_column\u0027, lit(\u0027This is a new column\u0027))\ndf_addcol.show()\n\nprint(\"*** Rename column\")\ndf_addcol \u003d df_addcol.withColumnRenamed(\u0027new_column\u0027, \u0027SUM\u0027)\ndf_addcol \u003d df_addcol.select(\"lu_name\", \"card_brand\",func.col(\u0027sum(dcc_bill_amount)\u0027).alias(\u0027sumAmount\u0027), func.col(\u0027sum(transaction_amount)\u0027).alias(\u0027sumTransaction\u0027), \"SUM\")\ndf_addcol.show(5)\n\nprint(\"*** Add calculated column\")\nnew_df \u003d df_addcol.withColumn(\u0027SUM\u0027, df_addcol.sumAmount + df_addcol.sumTransaction)\nnew_df.show(5)\nprint(\"*** Original data type\")\ndisplay(new_df)\n\nprint(\"\")\nprint(\"\")\nprint(\"*** Change type of column\")\nchangedTypedf \u003d new_df.withColumn(\"SUM\", new_df[\"SUM\"].cast(\"String\"))\ndisplay(changedTypedf)\n\nprint(\"\")\nprint(\"\")\nprint(\"*** Change type of column creating a new column\")\nchangedTypedf \u003d new_df.withColumn(\"SUM_STR\", new_df[\"SUM\"].cast(\"String\"))\ndisplay(changedTypedf)\n\nprint(\"\")\nprint(\"\")\nprint(\"*** Add column with condition\")\nnew_df.withColumn(\u0027Indicator\u0027, when(\n    (col(\"sumAmount\") \u003e 40000) \u0026 (col(\"sumAmount\")  \u003c\u003d100000), 4\n    ).when(col(\"sumAmount\") \u003e 100000, 2).otherwise(0)).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%pyspark\ncolumns \u003d [\"lu_name\",\"card_brand\", \"reason_code_description\", \"dcc_bill_amount\", \"transaction_amount\"]\ndata \u003d [(\"TE99999\", \"RBS\", \"DEFAULT\",0,0),\n    (\"TE99999\", \"VISA\", \"DEFAULT\",0,0),\n    (\"TE99999\", \"VISA\", \"EXPIRED\",0,0)]\n\ndf_newrow \u003d spark.createDataFrame(data\u003ddata,schema\u003dcolumns)\n\nfrom pyspark.sql import Row\ndf_newrow1 \u003d sc.parallelize([ \\\n    Row(lu_name\u003d\u0027TE88888\u0027, card_brand\u003d\u0027VISA\u0027, reason_code_description\u003d\u0027DEFAULT\u0027, dcc_bill_amount\u003d0, transaction_amount\u003d0), \\\n    Row(lu_name\u003d\u0027TE88888\u0027, card_brand\u003d\u0027VISA\u0027, reason_code_description\u003d\u0027EXPIRED\u0027, dcc_bill_amount\u003d0, transaction_amount\u003d0), \\\n    Row(lu_name\u003d\u0027TE88888\u0027, card_brand\u003d\u0027RBS\u0027, reason_code_description\u003d\u0027DEFAULT\u0027, dcc_bill_amount\u003d0, transaction_amount\u003d0)]).toDF()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%pyspark\nfrom pyspark.sql.functions import col\ndfbetween \u003d df.select(\"lu_name\", \"card_brand\",\"transaction_date_time\", \"dcc_bill_amount\", \"transaction_amount\") \\\n        .groupBy(\"lu_name\", \"card_brand\",\"transaction_date_time\").sum(\"dcc_bill_amount\",\"transaction_amount\") \\\n        .select(\"lu_name\", \"card_brand\",\"transaction_date_time\", col(\u0027sum(dcc_bill_amount)\u0027).alias(\u0027sumAmount\u0027), col(\u0027sum(transaction_amount)\u0027).alias(\u0027sumTransaction\u0027))\n\nprint(\"*** Filter using between\")\ndfbetween.select(\"lu_name\", \"card_brand\",\"transaction_date_time\",\"sumAmount\",\"sumTransaction\", dfbetween.sumTransaction.between(0, 100).alias(\u0027between\u0027)).filter(\u0027between\u0027).show()\n\ndfbetween.select(\"lu_name\", \"card_brand\",\"transaction_date_time\",\"sumAmount\",\"sumTransaction\", dfbetween.transaction_date_time.between(\u00272006-01-01\u0027, \u00272006-12-31\u0027).alias(\u0027between\u0027)).filter(\u0027between\u0027).show()\n\nprint(\"*** Filter using in\")\ndfbetween.where(col(\"card_brand\").isin({\"NATWES\", \"LINK\"})).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%pyspark\n"
    }
  ]
}