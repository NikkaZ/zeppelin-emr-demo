{
  "metadata": {
    "name": "01_Basic",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "List of functions shown in this notebook:\n - Show tables in a db\n - Show schema for a table\n - SQL Select\n - SQL Select Distinct\n - SQL Where\n - SQL And Or Not\n - SQL Order By\n - SQL Min and Max\n - SQL Count, Avg, Sum\n - SQL Group By\n - SQL Having\n - SQL Like\n - SQL Wildcards\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nshowtablesnames()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nshowschema(\"testNikka\")"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf \u003d runquery(\"select * from testNikka\")"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nprint(\"*** Select some columns\")\ndf.select(\u0027lu_name\u0027, \u0027card_brand\u0027, \u0027dcc_bill_amount\u0027).show(3)\n\nprint(\"*** Select some columns order by\")\ndf.select(\u0027lu_name\u0027, \u0027card_brand\u0027, \u0027dcc_bill_amount\u0027).orderBy(\u0027lu_name\u0027).show(3)\n\nprint(\"*** Select some columns sort. Order by is an alias of sort function https://stackoverflow.com/questions/40603202/what-is-the-difference-between-sort-and-orderby-functions-in-spark\")\ndf.select(\u0027lu_name\u0027, \u0027card_brand\u0027, \u0027dcc_bill_amount\u0027).sort(\u0027lu_name\u0027).show(3)\n\nprint(\"*** Select all but some columns\")\ncols \u003d list(set(df.columns) - {\u0027note_denom_20\u0027, \u0027YEAR\u0027})\ndf.select(cols).show(3)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%pyspark\nprint(\"*** Normal distinct\")\ndf.select(\u0027card_brand\u0027, \u0027issuer_institution_name\u0027).distinct().orderBy(\u0027card_brand\u0027).show()\n\nprint(\"*** drop duplicates, doesn\u0027t consider the other columns\")\ndf.select(\u0027card_brand\u0027, \u0027issuer_institution_name\u0027).drop_duplicates([\u0027card_brand\u0027]).orderBy(\u0027card_brand\u0027).show()\n\nprint(\"*** so this function needs to be carefully used. This an example that gives the same result of distinct\")\ndf.select(\u0027card_brand\u0027, \u0027issuer_institution_name\u0027).drop_duplicates([\u0027card_brand\u0027, \u0027issuer_institution_name\u0027]).orderBy(\u0027card_brand\u0027).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%pyspark\nprint(\"*** Filter selecting all columns\")\ndf.filter(df.lu_name \u003d\u003d \"TE20116\").show(2)\n\nprint(\"*** Filter with and\")\ndf.select(\u0027lu_name\u0027, \u0027card_brand\u0027, \u0027dcc_bill_amount\u0027).filter( (df.card_brand  \u003d\u003d \"VISA\") \u0026 (df.lu_name \u003d\u003d \"TE20116\") ).show(2)\n\nprint(\"*** Filter with and and or first method\")\ndf.select(\u0027lu_name\u0027, \u0027card_brand\u0027, \u0027dcc_bill_amount\u0027).filter((\u0027card_brand \u003d \"VISA\"\u0027)).filter(\u0027lu_name \u003d\u003d \"TE33806\" or lu_name \u003d \"TE37696\"\u0027).drop_duplicates([\u0027lu_name\u0027]).show()\n\nprint(\"*** Filter with and and or second method\")\ndf.select(\u0027lu_name\u0027, \u0027card_brand\u0027, \u0027dcc_bill_amount\u0027).filter(\u0027card_brand \u003d \"VISA\" AND (lu_name \u003d\u003d \"TE33806\" or lu_name \u003d \"TE37696\")\u0027).drop_duplicates([\u0027lu_name\u0027]).show()\n\nprint(\"*** Group by count\")\ndf.groupby(\u0027card_brand\u0027).count().show()\n\nprint(\"*** Group by count and order by desc\")\ndf.groupby(\u0027card_brand\u0027).count().orderBy(df.card_brand.desc()).show()\n\nprint(\"*** Order by asc\")\ndf.select(df.lu_name, df.issuer_country_code.alias(\u0027ICC\u0027)).orderBy(df.issuer_country_code.asc()).show()\n\nprint(\"*** Order by asc nulls last\")\ndf.select(df.lu_name, df.issuer_country_code.alias(\u0027ICC\u0027)).orderBy(df.issuer_country_code.asc_nulls_last()).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%pyspark\nfrom pyspark.sql.functions import col\nimport pyspark.sql.functions as func\n\nprint(\"*** Group by sum\")\ndf.groupby(\u0027card_brand\u0027).sum(\u0027dcc_bill_amount\u0027).show()\n\nprint(\"*** Group by alias\")\ndf.groupby(\u0027card_brand\u0027).sum(\u0027dcc_bill_amount\u0027).select(\u0027card_brand\u0027, func.col(\u0027sum(dcc_bill_amount)\u0027).alias(\u0027sumAmount\u0027)).show()\n\nprint(\"*** Group by alias and filter \u003e 0\")\ndf.groupby(\u0027card_brand\u0027).sum(\u0027dcc_bill_amount\u0027).select(\u0027card_brand\u0027, func.col(\u0027sum(dcc_bill_amount)\u0027).alias(\u0027sumAmount\u0027)).where(col(\u0027sum(dcc_bill_amount)\u0027)\u003e0).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%pyspark\nprint(\"*** Filter \u003d \")\ndf.select(\u0027lu_name\u0027).filter(df.lu_name \u003d\u003d \"TE20116\").show(2)\n\nprint(\"*** Filter like first method\")\ndf.select(\u0027lu_name\u0027).where(col(\u0027lu_name\u0027).like(\"%TE%\")).show(2)\n\nprint(\"*** Filter like second method\")\ndf.select(\u0027lu_name\u0027).where(\"lu_name like \u0027%TE%\u0027\").groupby(\u0027lu_name\u0027).count().show()\n\nprint(\"*** Filter with regolar expression for all the wildcards and more\")\nexpression \u003d r\u0027TE3..96\u0027\ndf.filter(df[\u0027lu_name\u0027].rlike(expression)).groupby(\u0027lu_name\u0027).count().show()"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%pyspark\nprint(\"*** Count\")\nprint(df.count())\nprint(\"\")\nprint(\"*** Number columns and names\")\nprint(len(df.columns), df.columns)\nprint(\"\")\nprint(\"*** Summary statistics (mean, standard deviance, min, max, count) of numerical columns\")\ndf.describe().show()\nprint(\"\")\nprint(\"*** Summary statistics on one or more column\")\ndf.describe(\u0027transaction_amount\u0027,\u0027note_denom_05\u0027,\u0027margin\u0027,\u0027reversal_amount\u0027,\u0027original_amount\u0027,\u0027dcc_bill_amount\u0027).show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%pyspark\nfrom pyspark.sql.functions import mean as _mean, stddev as _stddev, col, min, max\n\nprint(\"*** Second method\")\ndf.groupby(\u0027card_brand\u0027).agg({\u0027dcc_bill_amount\u0027: \u0027mean\u0027}).show()\n\nprint(\"\")\nprint(\"*** Third method\")\ndf_stats \u003d df.select(\n    _mean(col(\u0027dcc_bill_amount\u0027)).alias(\u0027mean\u0027),\n    _stddev(col(\u0027dcc_bill_amount\u0027)).alias(\u0027std\u0027)\n).collect()\n\nmean \u003d df_stats[0][\u0027mean\u0027]\nstd \u003d df_stats[0][\u0027std\u0027]\nprint(\"Mean \" + str(mean), \"Standard deviation \" + str(std))\n\nprint(\"\")\nprint(\"*** Forth method\")\ndf.select([_mean(\u0027dcc_bill_amount\u0027), min(\u0027dcc_bill_amount\u0027), max(\u0027dcc_bill_amount\u0027)]).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%pyspark\n"
    }
  ]
}