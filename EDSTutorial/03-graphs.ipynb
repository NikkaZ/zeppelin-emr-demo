{
  "metadata": {
    "name": "03-graphs",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql.functions import trim, col\n\ndf \u003d runquery(\"select * from testNikka\")\n\ndf_graphs \u003d df.withColumn(\"card_brand\", trim(col(\"card_brand\"))) \\\n                .withColumn(\"reason_code_description\", trim(col(\"reason_code_description\")))\ndf_graphs \u003d df_graphs.select(\"lu_name\", \"card_brand\", \"reason_code_description\", \"dcc_bill_amount\",\"transaction_amount\")\ndf_graphs.show()\n\ndf_graphs.createOrReplaceTempView(\"Transactions\")"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n\n\n\n\n\ndfs3 \u003d spark.read.load(\"s3a://eds-psf-s3-test-rbs-conformed/atm/rbs-atm-fault-test-conformed/year\u003d2020/month\u003d07/day\u003d17/run-datasink4-7-part-block-0-r-00000-snappy.parquet\")\ndfs3.show(6)\n\ndfs3.createOrReplaceTempView(\"Fault\")"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.sql\nselect card_brand, reason_code_description,dcc_bill_amount,transaction_amount from Transactions "
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.sql\nselect name, downtime_pot,fault_description from Fault"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nimport pyspark.sql.functions as func\nfrom pyspark.sql.functions import col, unix_timestamp, to_date\n\ndfs3_2010 \u003d spark.read.load(\"s3a://eds-psf-s3-test-rbs-conformed/atm/rbs-atm-transaction-test-conformed/year\u003d2021/month\u003d03/day\u003d01/*.parquet\")\n\ndfs3_2010 \u003d dfs3_2010.withColumn(\"card_brand\", trim(col(\"card_brand\"))) \\\n            .withColumn(\"lu_name\", trim(col(\"lu_name\")))\n\ndfs3_2010 \u003d dfs3_2010.groupby(\"lu_name\", \"card_brand\").sum(\"transaction_amount\").select(\"lu_name\", \"card_brand\", func.col(\u0027sum(transaction_amount)\u0027).alias(\u0027sumtransaction\u0027))\n\ndfs3_2020 \u003d spark.read.load(\"s3a://eds-psf-s3-test-rbs-conformed/atm/rbs-atm-transaction-test-conformed/year\u003d2030/month\u003d07/day\u003d15/*.parquet\")\n\ndfs3_2020 \u003d dfs3_2020.withColumn(\"card_brand\", trim(col(\"card_brand\"))) \\\n            .withColumn(\"lu_name\", trim(col(\"lu_name\")))\ndfs3_2020 \u003d dfs3_2020.groupby(\"lu_name\", \"card_brand\").sum(\"transaction_amount\").select(\"lu_name\", \"card_brand\", func.col(\u0027sum(transaction_amount)\u0027).alias(\u0027sumtransaction\u0027))\n\ndfs3_2010.createOrReplaceTempView(\"Transactions_2010\")\ndfs3_2020.createOrReplaceTempView(\"Transactions_2020\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.sql\nselect t2010.LU_NAME, t2010.CARD_BRAND, t2010.sumtransaction as sum2010, t2020.sumtransaction as sum2049 from Transactions_2010 as t2010\ninner join Transactions_2020 as t2020 on t2010.LU_NAME \u003d t2020.LU_NAME and t2010.CARD_BRAND \u003d t2020.CARD_BRAND"
    }
  ]
}