{
  "metadata": {
    "name": "00_InitialCode",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%pyspark\nimport boto3\nfrom botocore.config import Config\nssm \u003d boto3.client(\"ssm\", region_name\u003d\"eu-west-1\",config\u003dConfig(proxies\u003d{\"https\": \"proxy.service:8080\"}))\n\noracle_host\u003d\"tcps://teds0db.cibbtwlv7dnd.eu-west-1.rds.amazonaws.com\"\noracle_port\u003d\"2484\"\noracle_database\u003d\"TEDS0DB\"\noracle_url\u003d\"jdbc:oracle:thin:@\" + oracle_host + \":\" + oracle_port+ \"/\" + oracle_database + \"?javax.net.ssl.trustStore\u003d/usr/lib/oracle/19.6/client64/network/admin/wallet/cwallet.sso\"\noracle_username\u003d\"SVC_DATAENG_BUILD_TEST\"\noracle_password \u003d ssm.get_parameter(Name\u003d\u0027/rds/oracle/\u0027 + oracle_database + \u0027/\u0027 + oracle_username + \u0027_password\u0027, WithDecryption\u003dTrue)[\u0027Parameter\u0027][\u0027Value\u0027]"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%pyspark\ndef showschema(table):\n    df \u003d spark.read \\\n        .format(\"jdbc\") \\\n        .option(\"url\", oracle_url) \\\n        .option(\"dbtable\", table) \\\n        .option(\"user\", oracle_username) \\\n        .option(\"password\", oracle_password) \\\n        .option(\"driver\", \"oracle.jdbc.driver.OracleDriver\") \\\n        .load()\n    return df.printSchema()\n\ndef runquery(query):\n    return spark.read \\\n        .format(\"jdbc\") \\\n        .option(\"url\", oracle_url) \\\n        .option(\"query\", query) \\\n        .option(\"user\", oracle_username) \\\n        .option(\"password\", oracle_password) \\\n        .option(\"driver\", \"oracle.jdbc.driver.OracleDriver\") \\\n        .load()\n\ndef showtables():\n    return spark.read \\\n        .format(\"jdbc\") \\\n        .option(\"url\", oracle_url) \\\n        .option(\"query\", \"SELECT * FROM   all_tables WHERE TABLESPACE_NAME \u003d \u0027SVC_DATAENG_BUILD_TEST_TBS\u0027\") \\\n        .option(\"user\", oracle_username) \\\n        .option(\"password\", oracle_password) \\\n        .option(\"driver\", \"oracle.jdbc.driver.OracleDriver\") \\\n        .load()\n\ndef showtablesnames():\n    df \u003d spark.read \\\n        .format(\"jdbc\") \\\n        .option(\"url\", oracle_url) \\\n        .option(\"query\", \"SELECT TABLE_NAME FROM   all_tables WHERE TABLESPACE_NAME \u003d \u0027SVC_DATAENG_BUILD_TEST_TBS\u0027\") \\\n        .option(\"user\", oracle_username) \\\n        .option(\"password\", oracle_password) \\\n        .option(\"driver\", \"oracle.jdbc.driver.OracleDriver\") \\\n        .load()\n    return df.select(\"TABLE_NAME\").show()\n\ndef createtablefromdf(df, tablename, method):\n    df.write.format(\"jdbc\") \\\n        .mode(method) \\\n        .option(\"url\", oracle_url) \\\n        .option(\"dbtable\", tablename) \\\n        .option(\"user\", oracle_username) \\\n        .option(\"password\", oracle_password) \\\n        .save()"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nimport boto3\nfrom pyspark.sql.session import SparkSession\nfrom pyspark.context import SparkContext\nimport ntpath\ndef get_matching_s3_keys(bucket, prefix\u003d\u0027\u0027, suffix\u003d\u0027\u0027):\n    \"\"\"\n    Generate the keys in an S3 bucket.\n\n    :param bucket: Name of the S3 bucket.\n    :param prefix: Only fetch keys that start with this prefix (optional).\n    :param suffix: Only fetch keys that end with this suffix (optional).\n    \"\"\"\n    sc \u003d SparkContext.getOrCreate()\n    spark \u003d SparkSession(sc)\n    \n    s3resource \u003d boto3.resource(\"s3\", region_name\u003d\"eu-west-1\",config\u003dConfig(proxies\u003d{\"https\": \"proxy.service:8080\"}))\n    #buckets \u003d [bucket.name for bucket in s3.buckets.all()]\n    #print(buckets)\n    s3client \u003d boto3.client(\"s3\", region_name\u003d\"eu-west-1\",config\u003dConfig(proxies\u003d{\"https\": \"proxy.service:8080\"}))\n    dict_files\u003d{}\n    kwargs \u003d {\u0027Bucket\u0027: bucket, \u0027Prefix\u0027: prefix}\n    while True:\n        resp \u003d s3client.list_objects_v2(**kwargs)\n        for obj in resp[\u0027Contents\u0027]:\n            key \u003d obj[\u0027Key\u0027]\n            if key.endswith(suffix):\n                head, tail \u003d ntpath.split(key)\n                if head not in dict_files:\n                    dict_files[head] \u003d [tail]\n                else:\n                    dict_files[head].append(tail)\n        return dict_files\n        try:\n            kwargs[\u0027ContinuationToken\u0027] \u003d resp[\u0027NextContinuationToken\u0027]\n        except KeyError:\n            break\n"
    }
  ]
}