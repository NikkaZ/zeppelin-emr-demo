{
  "metadata": {
    "name": "_Process_Overview",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "This notebook uses pre-signed POST requests. \nThe request needs to be generate first and the upload form updated with the pre-signed POST request details.\nPlease run the first two paragrahs before uploading to s3"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nbucket_name\u003d\"eds-psf-s3-test-spark-cluster-dataeng-build\"\nobject_name\u003d\"uploads/${filename}\"\nfields\u003dNone\nconditions\u003dNone\nexpiration\u003d3600\n\nimport boto3\n\ns3_client \u003d boto3.client(\u0027s3\u0027, region_name\u003d\"eu-west-1\", config\u003dboto3.session.Config(signature_version\u003d\u0027s3v4\u0027))\n\nresponse \u003d s3_client.generate_presigned_post(\n    bucket_name,\n    object_name,\n    Fields\u003dfields,\n    Conditions\u003dconditions,\n    ExpiresIn\u003dexpiration\n)\n\nurl\u003dresponse[\u0027url\u0027]\nz.remove(url)\nz.put(\"url\", url)\n\nkey\u003dresponse[\u0027fields\u0027][\u0027key\u0027]\nz.put(\"key\", key)\n\nx_amz_credential\u003dresponse[\u0027fields\u0027][\u0027x-amz-credential\u0027]\nz.put(\"x_amz_credential\", x_amz_credential)\n\nx_amz_security_token\u003dresponse[\u0027fields\u0027][\u0027x-amz-security-token\u0027]\nz.put(\"x_amz_security_token\", x_amz_security_token)\n\npolicy\u003dresponse[\u0027fields\u0027][\u0027policy\u0027]\nz.put(\"policy\", policy)\n\nx_amz_signature\u003dresponse[\u0027fields\u0027][\u0027x-amz-signature\u0027]\nz.put(\"x_amz_signature\", x_amz_signature)\n\nx_amz_date\u003dresponse[\u0027fields\u0027][\u0027x-amz-date\u0027]\nz.put(\"x_amz_date\", x_amz_date)\n\n#x-amz-algorithm\n\nprint(response)"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n{\nval url\u003dz.get(\"url\")\nz.angularBind(\"url\", url)\n\nval object_prefix\u003dz.get(\"object_prefix\")\nz.angularBind(\"object_prefix\", object_prefix)\n\nval policy\u003dz.get(\"policy\")\nz.angularBind(\"policy\", policy)\n\nval x_amz_credential\u003dz.get(\"x_amz_credential\")\nz.angularBind(\"x_amz_credential\",x_amz_credential)\n\nval x_amz_security_token\u003dz.get(\"x_amz_security_token\")\nz.angularBind(\"x_amz_security_token\",x_amz_security_token)\n\nval x_amz_signature\u003dz.get(\"x_amz_signature\")\nz.angularBind(\"x_amz_signature\",x_amz_signature)\n\nval x_amz_date\u003dz.get(\"x_amz_date\")\nz.angularBind(\"x_amz_date\",x_amz_date)\n\n\nprintln(\"\"\"\n%angular\n\u003chtml\u003e\n\u003cscript\u003e\n    function showname () {\n      var file_name \u003d document.getElementById(\u0027file_input\u0027).files.item(0).name; \n      document.getElementById(\u0027key_input\u0027).value\u003d\"uploads/\"+file_name;\n    };\n\u003c/script\u003e\n  \u003chead\u003e\n    \u003cmeta http-equiv\u003d\"Content-Type\" content\u003d\"text/html; charset\u003dUTF-8\" /\u003e\n  \u003c/head\u003e\n  \u003cbody\u003e\n  \u003cform action\u003d\"https://eds-psf-s3-test-spark-cluster-dataeng-build.s3.amazonaws.com/\" method\u003d\"post\" enctype\u003d\"multipart/form-data\"\u003e\n    \u003cinput id\u003d\"key_input\" type\u003d\"hidden\" name\u003d\"key\" value\u003d\"\" /\u003e\u003cbr /\u003e\n    \u003cinput type\u003d\"hidden\" name\u003d\"X-Amz-Algorithm\" value\u003d\"AWS4-HMAC-SHA256\" /\u003e\n    \u003cinput type\u003d\"hidden\" name\u003d\"x-amz-security-token\" value\u003d\"{{x_amz_security_token}}\" /\u003e\n    \u003cinput type\u003d\"hidden\" name\u003d\"X-Amz-Credential\" value\u003d\"{{x_amz_credential}}\" /\u003e\n    \u003cinput type\u003d\"hidden\" name\u003d\"X-Amz-Date\" value\u003d\"{{x_amz_date}}\" /\u003e\n    \u003cinput type\u003d\"hidden\" name\u003d\"X-Amz-Signature\" value\u003d\"{{x_amz_signature}}\" /\u003e\n    \u003cinput type\u003d\"hidden\" name\u003d\"Policy\" value\u003d\"{{policy}}\" /\u003e\n    \u003cinput  id\u003d\"file_input\" type\u003d\"file\" name\u003d\"file\" onChange\u003d\"showname()\"/\u003e\n    \u003cinput type\u003d\"submit\" name\u003d\"submit\" value\u003d\"Upload to Amazon S3\" /\u003e\n  \u003c/form\u003e\n \n\u003c/html\u003e\n\"\"\")\n}\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nimport pprint \ndictfiles \u003d get_matching_s3_keys(bucket\u003d\u0027eds-psf-s3-test-spark-cluster-dataeng-build\u0027, prefix\u003d\u0027\u0027, suffix\u003d\u0027.csv\u0027)\npp \u003d pprint.PrettyPrinter(indent\u003d4)\npp.pprint(dictfiles)"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nimport pandas as pd\n \ndf \u003d pd.read_csv(\u0027s3a://eds-psf-s3-test-spark-cluster-dataeng-build/uploads/daily_faultdata_20970909.csv\u0027, sep\u003d\u0027,\u0027, header\u003d0)\ndf"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf \u003d spark.read.format(\u0027csv\u0027).options(header\u003d\u0027true\u0027).load(\"s3a://eds-psf-s3-test-spark-cluster-dataeng-build/uploads/daily_faultdata_20970909.csv\")\ndf.count()\ncreatetablefromdf(df, \"daily_faultdata\", \"overwrite\")"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%teds0db\ndrop table NZ_DF1"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%teds0db\nSELECT TABLE_NAME FROM all_tables WHERE TABLESPACE_NAME \u003d \u0027SVC_DATAENG_BUILD_TEST_TBS\u0027 order by TABLE_NAME"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%teds0db\nselect * from daily_faultdata WHERE ROWNUM \u003c\u003d 10\n"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf \u003d runquery(\"select * from DAILY_FAULTDATA\")"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nimport pandas as pd\nfrom IPython.display import HTML\nfrom pyspark.context import SparkContext\n\n\ndef create_download_link( df, title \u003d \"Download CSV file\", filename \u003d \"daily_faultdata.csv\"):  \n    csv \u003d df.to_csv()\n    b64 \u003d base64.b64encode(csv.encode())\n    payload \u003d b64.decode()\n    html \u003d \u0027\u003ca download\u003d\"{filename}\" href\u003d\"data:text/csv;base64,{payload}\" target\u003d\"_blank\"\u003e{title}\u003c/a\u003e\u0027\n    html \u003d html.format(payload\u003dpayload,title\u003dtitle,filename\u003dfilename)\n    return HTML(html)\n\ndf_download \u003d df.select(\u0027Device ID\u0027, \u0027Duration\u0027, \u0027estate\u0027, \u0027severity\u0027, \u0027Downtime\u0027)\ndf_download.show(3)\n\ndf_download.printSchema()\ndf_download.show(truncate\u003dFalse)\npdf\u003ddf_download.toPandas()\n\ncreate_download_link(pdf)"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndfstran \u003d spark.read.load(\"s3a://eds-psf-s3-test-rbs-conformed/atm/rbs-atm-transaction-test-conformed/year\u003d2021/month\u003d03/day\u003d01/run-datasink4-13-part-block-0-0-r-00000-snappy.parquet\")\ncreatetablefromdf(dfstran, \"testNikka\", \"overwrite\")"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}